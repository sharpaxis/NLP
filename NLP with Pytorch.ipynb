{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcf4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5189654",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/shakespeare.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3ea041",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_char = set(text)\n",
    "n_unique_char = len(all_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a3cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_char))\n",
    "encoder = {char:ind for ind,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad8573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1829a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text,n_unique_char):\n",
    "    one_hot = np.zeros((encoded_text.size,n_unique_char)).astype(np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]),encoded_text.flatten()] = 1.0\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape,n_unique_char))\n",
    "    return one_hot   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90b1a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text,sample_per_batch=10,seq_len=50):\n",
    "    char_per_batch = sample_per_batch * seq_len\n",
    "    avail_batch = int(len(encoded_text)/char_per_batch)\n",
    "    encoded_text = encoded_text[:char_per_batch*avail_batch]\n",
    "    encoded_text = encoded_text.reshape((sample_per_batch,-1))\n",
    "    \n",
    "    for n in range(0,encoded_text.shape[1],seq_len):\n",
    "        x = encoded_text[:,n:n+seq_len]\n",
    "        y = np.zeros_like(x)\n",
    "        try : \n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_text[:,n+seq_len]\n",
    "        #for the very last case\n",
    "        except : \n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_text[:,0]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a7b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(encoded_text,sample_per_batch=10,seq_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17e222bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f39f277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,all_char,num_hidden=256,num_layers=4,drop_prob=0.5):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.all_char = all_char\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        self.decoder = dict(enumerate(all_char))\n",
    "        self.encoder = {char:ind for ind,char in decoder.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_char),num_hidden,num_layers,dropout = drop_prob,batch_first=True)\n",
    "        self.fc_linear = nn.Linear(num_hidden,len(self.all_char))\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "    def forward(self,x,hidden):\n",
    "        lstm_out, hidden = self.lstm(x,hidden)\n",
    "        drop_out = self.dropout(lstm_out)\n",
    "        drop_out = drop_out.contiguous().view(-1,self.num_hidden)\n",
    "        final_out = self.fc_linear(drop_out)\n",
    "        return final_out,hidden\n",
    "    def init_hidden(self,batch_size):\n",
    "        hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).to(device),\n",
    "                 torch.zeros(self.num_layers,batch_size,self.num_hidden).to(device))\n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce402d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(all_char,num_hidden=256,num_layers=4,drop_prob=0.4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2884637b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(84, 256, num_layers=4, batch_first=True, dropout=0.4)\n",
       "  (fc_linear): Linear(in_features=256, out_features=84, bias=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31ab2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f420852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.9\n",
    "train_ind = int(len(encoded_text) * (train_percent))\n",
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8313ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 75\n",
    "batch_size = 100\n",
    "seq_len = 100\n",
    "tracker = 0\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ed67527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(84, 256, num_layers=4, batch_first=True, dropout=0.4)\n",
       "  (fc_linear): Linear(in_features=256, out_features=84, bias=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78187ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  : 1 step : 25 val_loss : 3.2059872150421143\n",
      "epoch  : 1 step : 50 val_loss : 3.197155475616455\n",
      "epoch  : 1 step : 75 val_loss : 3.1957905292510986\n",
      "epoch  : 1 step : 100 val_loss : 3.1974141597747803\n",
      "epoch  : 1 step : 125 val_loss : 3.1971182823181152\n",
      "epoch  : 1 step : 150 val_loss : 3.1970865726470947\n",
      "epoch  : 1 step : 175 val_loss : 3.1945366859436035\n",
      "epoch  : 1 step : 200 val_loss : 3.1953234672546387\n",
      "epoch  : 1 step : 225 val_loss : 3.1963179111480713\n",
      "epoch  : 1 step : 250 val_loss : 3.199916362762451\n",
      "epoch  : 1 step : 275 val_loss : 3.1965813636779785\n",
      "epoch  : 1 step : 300 val_loss : 3.1970553398132324\n",
      "epoch  : 1 step : 325 val_loss : 3.1973752975463867\n",
      "epoch  : 1 step : 350 val_loss : 3.196497678756714\n",
      "epoch  : 1 step : 375 val_loss : 3.1967051029205322\n",
      "epoch  : 1 step : 400 val_loss : 3.1948065757751465\n",
      "epoch  : 1 step : 425 val_loss : 3.193148136138916\n",
      "epoch  : 1 step : 450 val_loss : 3.1941583156585693\n",
      "epoch  : 1 step : 475 val_loss : 3.1933465003967285\n",
      "epoch  : 2 step : 500 val_loss : 3.189988374710083\n",
      "epoch  : 2 step : 525 val_loss : 3.175462484359741\n",
      "epoch  : 2 step : 550 val_loss : 3.116403579711914\n",
      "epoch  : 2 step : 575 val_loss : 3.0108237266540527\n",
      "epoch  : 2 step : 600 val_loss : 2.909346342086792\n",
      "epoch  : 2 step : 625 val_loss : 2.8362414836883545\n",
      "epoch  : 2 step : 650 val_loss : 2.761463165283203\n",
      "epoch  : 2 step : 675 val_loss : 2.6996943950653076\n",
      "epoch  : 2 step : 700 val_loss : 2.640096664428711\n",
      "epoch  : 2 step : 725 val_loss : 2.5998876094818115\n",
      "epoch  : 2 step : 750 val_loss : 2.571183443069458\n",
      "epoch  : 2 step : 775 val_loss : 2.5434820652008057\n",
      "epoch  : 2 step : 800 val_loss : 2.5108413696289062\n",
      "epoch  : 2 step : 825 val_loss : 2.501122236251831\n",
      "epoch  : 2 step : 850 val_loss : 2.472954750061035\n",
      "epoch  : 2 step : 875 val_loss : 2.4543745517730713\n",
      "epoch  : 2 step : 900 val_loss : 2.4384262561798096\n",
      "epoch  : 2 step : 925 val_loss : 2.426635265350342\n",
      "epoch  : 2 step : 950 val_loss : 2.4152188301086426\n",
      "epoch  : 2 step : 975 val_loss : 2.402308464050293\n",
      "epoch  : 3 step : 1000 val_loss : 2.4021129608154297\n",
      "epoch  : 3 step : 1025 val_loss : 2.385399580001831\n",
      "epoch  : 3 step : 1050 val_loss : 2.3816027641296387\n",
      "epoch  : 3 step : 1075 val_loss : 2.3693368434906006\n",
      "epoch  : 3 step : 1100 val_loss : 2.3566417694091797\n",
      "epoch  : 3 step : 1125 val_loss : 2.349208116531372\n",
      "epoch  : 3 step : 1150 val_loss : 2.343524217605591\n",
      "epoch  : 3 step : 1175 val_loss : 2.3376212120056152\n",
      "epoch  : 3 step : 1200 val_loss : 2.3421812057495117\n",
      "epoch  : 3 step : 1225 val_loss : 2.3244192600250244\n",
      "epoch  : 3 step : 1250 val_loss : 2.3204667568206787\n",
      "epoch  : 3 step : 1275 val_loss : 2.3175318241119385\n",
      "epoch  : 3 step : 1300 val_loss : 2.3113608360290527\n",
      "epoch  : 3 step : 1325 val_loss : 2.3118836879730225\n",
      "epoch  : 3 step : 1350 val_loss : 2.307548761367798\n",
      "epoch  : 3 step : 1375 val_loss : 2.304208517074585\n",
      "epoch  : 3 step : 1400 val_loss : 2.303354024887085\n",
      "epoch  : 3 step : 1425 val_loss : 2.2962262630462646\n",
      "epoch  : 3 step : 1450 val_loss : 2.2979323863983154\n",
      "epoch  : 4 step : 1475 val_loss : 2.295623779296875\n",
      "epoch  : 4 step : 1500 val_loss : 2.2855587005615234\n",
      "epoch  : 4 step : 1525 val_loss : 2.297907829284668\n",
      "epoch  : 4 step : 1550 val_loss : 2.2835774421691895\n",
      "epoch  : 4 step : 1575 val_loss : 2.2775368690490723\n",
      "epoch  : 4 step : 1600 val_loss : 2.281813621520996\n",
      "epoch  : 4 step : 1625 val_loss : 2.2715096473693848\n",
      "epoch  : 4 step : 1650 val_loss : 2.2757809162139893\n",
      "epoch  : 4 step : 1675 val_loss : 2.2682347297668457\n",
      "epoch  : 4 step : 1700 val_loss : 2.279418706893921\n",
      "epoch  : 4 step : 1725 val_loss : 2.270920753479004\n",
      "epoch  : 4 step : 1750 val_loss : 2.2649669647216797\n",
      "epoch  : 4 step : 1775 val_loss : 2.263195037841797\n",
      "epoch  : 4 step : 1800 val_loss : 2.2672173976898193\n",
      "epoch  : 4 step : 1825 val_loss : 2.2619287967681885\n",
      "epoch  : 4 step : 1850 val_loss : 2.257730484008789\n",
      "epoch  : 4 step : 1875 val_loss : 2.2641196250915527\n",
      "epoch  : 4 step : 1900 val_loss : 2.253391981124878\n",
      "epoch  : 4 step : 1925 val_loss : 2.247891902923584\n",
      "epoch  : 4 step : 1950 val_loss : 2.2540040016174316\n",
      "epoch  : 5 step : 1975 val_loss : 2.247922658920288\n",
      "epoch  : 5 step : 2000 val_loss : 2.2445285320281982\n",
      "epoch  : 5 step : 2025 val_loss : 2.2376110553741455\n",
      "epoch  : 5 step : 2050 val_loss : 2.2418017387390137\n",
      "epoch  : 5 step : 2075 val_loss : 2.242199659347534\n",
      "epoch  : 5 step : 2100 val_loss : 2.244497537612915\n",
      "epoch  : 5 step : 2125 val_loss : 2.2430241107940674\n",
      "epoch  : 5 step : 2150 val_loss : 2.2315614223480225\n",
      "epoch  : 5 step : 2175 val_loss : 2.2378883361816406\n",
      "epoch  : 5 step : 2200 val_loss : 2.2369441986083984\n",
      "epoch  : 5 step : 2225 val_loss : 2.231712818145752\n",
      "epoch  : 5 step : 2250 val_loss : 2.226285934448242\n",
      "epoch  : 5 step : 2275 val_loss : 2.2370526790618896\n",
      "epoch  : 5 step : 2300 val_loss : 2.2223057746887207\n",
      "epoch  : 5 step : 2325 val_loss : 2.226807117462158\n",
      "epoch  : 5 step : 2350 val_loss : 2.222407341003418\n",
      "epoch  : 5 step : 2375 val_loss : 2.224705219268799\n",
      "epoch  : 5 step : 2400 val_loss : 2.219900369644165\n",
      "epoch  : 5 step : 2425 val_loss : 2.217135429382324\n",
      "epoch  : 5 step : 2450 val_loss : 2.210601329803467\n",
      "epoch  : 6 step : 2475 val_loss : 2.2160606384277344\n",
      "epoch  : 6 step : 2500 val_loss : 2.214581251144409\n",
      "epoch  : 6 step : 2525 val_loss : 2.2076034545898438\n",
      "epoch  : 6 step : 2550 val_loss : 2.207310199737549\n",
      "epoch  : 6 step : 2575 val_loss : 2.208355188369751\n",
      "epoch  : 6 step : 2600 val_loss : 2.201903820037842\n",
      "epoch  : 6 step : 2625 val_loss : 2.2079918384552\n",
      "epoch  : 6 step : 2650 val_loss : 2.200924873352051\n",
      "epoch  : 6 step : 2675 val_loss : 2.1962599754333496\n",
      "epoch  : 6 step : 2700 val_loss : 2.1998705863952637\n",
      "epoch  : 6 step : 2725 val_loss : 2.202875852584839\n",
      "epoch  : 6 step : 2750 val_loss : 2.191427707672119\n",
      "epoch  : 6 step : 2775 val_loss : 2.199829578399658\n",
      "epoch  : 6 step : 2800 val_loss : 2.1948399543762207\n",
      "epoch  : 6 step : 2825 val_loss : 2.192129611968994\n",
      "epoch  : 6 step : 2850 val_loss : 2.1911847591400146\n",
      "epoch  : 6 step : 2875 val_loss : 2.189232587814331\n",
      "epoch  : 6 step : 2900 val_loss : 2.1934754848480225\n",
      "epoch  : 6 step : 2925 val_loss : 2.1769163608551025\n",
      "epoch  : 7 step : 2950 val_loss : 2.1839518547058105\n",
      "epoch  : 7 step : 2975 val_loss : 2.18493914604187\n",
      "epoch  : 7 step : 3000 val_loss : 2.184706926345825\n",
      "epoch  : 7 step : 3025 val_loss : 2.181901216506958\n",
      "epoch  : 7 step : 3050 val_loss : 2.1800363063812256\n",
      "epoch  : 7 step : 3075 val_loss : 2.186223030090332\n",
      "epoch  : 7 step : 3100 val_loss : 2.1813619136810303\n",
      "epoch  : 7 step : 3125 val_loss : 2.18234920501709\n",
      "epoch  : 7 step : 3150 val_loss : 2.179292678833008\n",
      "epoch  : 7 step : 3175 val_loss : 2.180480480194092\n",
      "epoch  : 7 step : 3200 val_loss : 2.177887201309204\n",
      "epoch  : 7 step : 3225 val_loss : 2.1746323108673096\n",
      "epoch  : 7 step : 3250 val_loss : 2.1646625995635986\n",
      "epoch  : 7 step : 3275 val_loss : 2.1791117191314697\n",
      "epoch  : 7 step : 3300 val_loss : 2.1721770763397217\n",
      "epoch  : 7 step : 3325 val_loss : 2.1729838848114014\n",
      "epoch  : 7 step : 3350 val_loss : 2.168991804122925\n",
      "epoch  : 7 step : 3375 val_loss : 2.1667416095733643\n",
      "epoch  : 7 step : 3400 val_loss : 2.162768840789795\n",
      "epoch  : 7 step : 3425 val_loss : 2.164189338684082\n",
      "epoch  : 8 step : 3450 val_loss : 2.164421796798706\n",
      "epoch  : 8 step : 3475 val_loss : 2.1699447631835938\n",
      "epoch  : 8 step : 3500 val_loss : 2.1653246879577637\n",
      "epoch  : 8 step : 3525 val_loss : 2.1741397380828857\n",
      "epoch  : 8 step : 3550 val_loss : 2.1629385948181152\n",
      "epoch  : 8 step : 3575 val_loss : 2.156083345413208\n",
      "epoch  : 8 step : 3600 val_loss : 2.1547598838806152\n",
      "epoch  : 8 step : 3625 val_loss : 2.1544535160064697\n",
      "epoch  : 8 step : 3650 val_loss : 2.1693942546844482\n",
      "epoch  : 8 step : 3675 val_loss : 2.1529221534729004\n",
      "epoch  : 8 step : 3700 val_loss : 2.150125741958618\n",
      "epoch  : 8 step : 3725 val_loss : 2.1549324989318848\n",
      "epoch  : 8 step : 3750 val_loss : 2.1550683975219727\n",
      "epoch  : 8 step : 3775 val_loss : 2.1611597537994385\n",
      "epoch  : 8 step : 3800 val_loss : 2.152595281600952\n",
      "epoch  : 8 step : 3825 val_loss : 2.154566764831543\n",
      "epoch  : 8 step : 3850 val_loss : 2.1530284881591797\n",
      "epoch  : 8 step : 3875 val_loss : 2.1492176055908203\n",
      "epoch  : 8 step : 3900 val_loss : 2.146235466003418\n",
      "epoch  : 9 step : 3925 val_loss : 2.1646008491516113\n",
      "epoch  : 9 step : 3950 val_loss : 2.1419341564178467\n",
      "epoch  : 9 step : 3975 val_loss : 2.1563515663146973\n",
      "epoch  : 9 step : 4000 val_loss : 2.1441686153411865\n",
      "epoch  : 9 step : 4025 val_loss : 2.150646448135376\n",
      "epoch  : 9 step : 4050 val_loss : 2.1493935585021973\n",
      "epoch  : 9 step : 4075 val_loss : 2.142808675765991\n",
      "epoch  : 9 step : 4100 val_loss : 2.144376277923584\n",
      "epoch  : 9 step : 4125 val_loss : 2.136702060699463\n",
      "epoch  : 9 step : 4150 val_loss : 2.1451096534729004\n",
      "epoch  : 9 step : 4175 val_loss : 2.1423914432525635\n",
      "epoch  : 9 step : 4200 val_loss : 2.131807327270508\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m     lstm_output,val_hidden \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(inputs,val_hidden)\n\u001b[1;32m     26\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m criterion(lstm_output,targets\u001b[38;5;241m.\u001b[39mview(batch_size\u001b[38;5;241m*\u001b[39mseq_len)\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m---> 27\u001b[0m     val_losses\u001b[38;5;241m.\u001b[39mappend(val_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch  : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m step : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtracker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m val_loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        tracker +=1 \n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        inputs = torch.tensor(x).to(device)\n",
    "        targets = torch.LongTensor(y).to(device)\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        model.zero_grad()\n",
    "        lstm_out,hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_out,targets.view(batch_size*seq_len).long())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        optimizer.step()\n",
    "        if tracker % 25 == 0:\n",
    "            val_hidden = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                inputs = torch.tensor(x).to(device)\n",
    "                targets = torch.LongTensor(y).to(device)\n",
    "                val_hidden = tuple([state.data for state in hidden])\n",
    "                lstm_output,val_hidden = model.forward(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "                val_losses.append(val_loss.item())\n",
    "            model.train()\n",
    "            print(f\"epoch  : {epoch+1} step : {tracker} val_loss : {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c502de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
